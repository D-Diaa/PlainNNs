import numpy as np
from scipy.linalg import eigh
from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


def SpH(data, maxbits, n_iter=100):
    N, D = data.shape
    centers = random_center(data, maxbits)
    O1, O2, radii, avg, stddev = compute_statistics(data, centers)

    iter_count = 1
    while True:
        forces = np.zeros((maxbits, D))
        for i in range(maxbits - 1):
            for j in range(i + 1, maxbits):
                force = 0.5 * (O2[i, j] - N / 4) / (N / 4) * (centers[i] - centers[j])
                forces[i] += force / maxbits
                forces[j] -= force / maxbits

        centers += forces
        O1, O2, radii, avg, stddev = compute_statistics(data, centers)

        if avg <= 0.1 * N / 4 and stddev <= 0.15 * N / 4:
            break
        if iter_count >= n_iter:
            print(f'Iteration exceeded {n_iter}, avg = {avg}, stddev = {stddev}')
            break
        iter_count += 1

    # print(f'Iteration count = {iter_count}')
    model = {
        'centers': centers,
        'radii': radii
    }
    return model, SpH_compress(data, model)


def random_center(data, bit):
    N, D = data.shape
    centers = np.zeros((bit, D))
    for i in range(bit):
        R = np.random.permutation(N)
        sample = data[R[:100]]
        centers[i] = np.mean(sample, axis=0)
    return centers


def compute_statistics(data, centers):
    N, D = data.shape
    bit = centers.shape[0]

    # Compute Euclidean distances between centers and data points
    dist = euclidean_distance(centers, data)  # Shape: (bit, N)
    sort_dist = np.sort(dist, axis=1)
    radii = sort_dist[:, N // 2]  # Median distance for each center

    # Determine if data points are inside spheres
    dist_binary = dist <= radii[:, np.newaxis]
    dist_binary = dist_binary.astype(float)

    O1 = np.sum(dist_binary, axis=1)
    O2 = dist_binary @ dist_binary.T  # Overlaps between spheres

    # Compute average absolute difference and average overlap
    indices_upper = np.triu_indices(bit, k=1)
    overlaps = O2[indices_upper]
    avg = np.mean(np.abs(overlaps - N / 4))
    avg2 = np.mean(overlaps)

    # Compute standard deviation of overlaps
    stddev = np.sqrt(np.mean((overlaps - avg2) ** 2))
    return O1, O2, radii, avg, stddev


def euclidean_distance(A, B):
    """
    Efficient computation of Euclidean distance between two sets of vectors.

    Parameters:
    - A: M x D numpy array
    - B: N x D numpy array

    Returns:
    - distances: M x N numpy array where distances[i, j] is the distance between A[i] and B[j]
    """
    A_squared = np.sum(A ** 2, axis=1).reshape(-1, 1)
    B_squared = np.sum(B ** 2, axis=1)
    distances = np.sqrt(A_squared - 2 * A @ B.T + B_squared)
    return distances


def SpH_compress(A: np.ndarray, model: dict):
    """
    Spherical Hashing (SpH) Compression Function.

    Parameters
    ----------
    A : np.ndarray
        Data matrix to compress, shape (n_samples, n_features). Each row represents a sample vector.
        The data should be normalized and centralized to have zero mean before.
    model : dict
        The model generated by `spherical_hashing`, containing:
            - 'centers': Learned centers, shape (bit, n_features).
            - 'radii': Radii for each center, shape (bit,).

    Returns
    -------
    B : np.ndarray
        Binary codes matrix, shape (n_samples, bit). Each row corresponds to a sample's binary code.
    """

    centers = model['centers']  # Shape: (bit, n_features)
    radii = model['radii']  # Shape: (bit,)

    # Compute distances between A and centers
    D = euclidean_distance(A, centers)  # Shape: (n_samples, bit)

    # Generate binary codes: 1 if distance > radius, else 0
    B = (D > radii[np.newaxis, :]).astype(int)

    return B


def DSH(X: np.ndarray, maxbits: int):
    alpha = maxbits
    r = maxbits
    iter_max = 7
    cluster = round(maxbits * alpha)

    # Perform K-Means clustering
    kmeans = KMeans(n_clusters=cluster, max_iter=iter_max)
    dump = kmeans.fit_predict(X)
    U = kmeans.cluster_centers_  # Shape: (cluster, n_features)

    Nsamples, Nfeatures = X.shape

    # Initialize model
    model = {
        'U': np.zeros((maxbits, Nfeatures)),
        'intercept': np.zeros(maxbits)
    }

    # Calculate cluster sizes and normalize
    clusize = np.array([(dump == i).sum() for i in range(cluster)], dtype=float)
    clusize /= clusize.sum()

    # Compute pairwise Euclidean distances between cluster centers
    Du = cdist(U, U, 'euclidean')  # Shape: (cluster, cluster)
    np.fill_diagonal(Du, np.inf)  # Set diagonal to infinity

    # Find the r nearest distances for each cluster
    Dr = []
    for i in range(Du.shape[0]):
        tmp = Du[i, :]
        sorted_indices = np.argsort(tmp)
        sorted_distances = tmp[sorted_indices]
        Dr.extend(sorted_distances[:r])
    Dr = np.unique(Dr)

    Dr = Dr.reshape(-1, 1)  # Ensure Dr is a column vector

    # Calculate bitsize for each unique distance in Dr
    bitsize = np.zeros(len(Dr))
    for i in range(len(Dr)):
        distance = Dr[i]
        id1, id2 = np.where(Du == distance)[0][0], np.where(Du == distance)[1][0]

        # Compute the average and difference of the selected cluster centers
        tmp1 = (U[id1, :] + U[id2, :]) / 2.0  # Shape: (n_features,)
        tmp2 = (U[id1, :] - U[id2, :]).reshape(-1, 1)  # Shape: (n_features, 1)

        tmp3 = np.tile(tmp1, (U.shape[0], 1))  # Shape: (cluster, n_features)

        # Compute projections
        DD = U @ tmp2  # Shape: (cluster, 1)
        th = tmp3 @ tmp2  # Shape: (cluster, 1)

        # Determine which projections are greater than the threshold
        pnum = (DD > th).flatten()  # Shape: (cluster,)

        # Calculate the density ratio
        tmpnum = clusize[pnum]
        num1 = tmpnum.sum()
        num2 = 1.0 - num1
        bitsize[i] = min(num1, num2) / max(num1, num2) if max(num1, num2) > 0 else 0

    # Sort bitsize in descending order and select top 'maxbits'
    sorted_indices = np.argsort(-bitsize)  # Descending order
    selected_indices = sorted_indices[:maxbits]

    for i, idx in enumerate(selected_indices):
        distance = Dr[idx]
        id1, id2 = np.where(Du == distance)[0][0], np.where(Du == distance)[1][0]

        # Update Du to prevent selecting the same pair again
        Du[id1, id2] = np.inf
        Du[id2, id1] = np.inf
        bitsize[idx] = np.inf

        # Set projection vector U[i] and intercept[i]
        model['U'][i, :] = U[id1, :] - U[id2, :]
        model['intercept'][i] = np.dot((U[id1, :] + U[id2, :]) / 2.0, model['U'][i, :])

    # Compute binary codes
    Ym = X @ model['U'].T  # Shape: (n_samples, maxbits)
    res = np.tile(model['intercept'], (Nsamples, 1))  # Shape: (n_samples, maxbits)
    B = (Ym > res).astype(int)  # Binary matrix: 1 if Ym > res, else 0

    return model, B


def ITQ(X: np.ndarray, maxbits, n_iter: int = 50) -> (np.ndarray, np.ndarray):
    pca = PCA(n_components=maxbits)
    V = pca.fit_transform(X)
    n_samples, n_bits = V.shape

    # Initialize with a random orthogonal rotation
    R = np.random.randn(n_bits, n_bits)
    U, _, _ = np.linalg.svd(R)
    R = U[:, :n_bits]
    UX = np.ones_like(V) * -1
    # ITQ optimization to find the optimal rotation
    for _ in range(n_iter):
        Z = V @ R
        UX = np.ones_like(Z) * -1
        UX[Z >= 0] = 1

        C = UX.T @ V
        UB, _, UA = np.linalg.svd(C)
        R = UA @ UB.T
    UX[UX < 0] = 0
    model = {
        'pca': pca,
        'R': R
    }
    return model, UX.astype(int)


def ISOH(A: np.ndarray, maxbits: int, n_iter: int = 100) -> (np.ndarray, np.ndarray):
    # Step 1: Compute A^T * A
    ATA = A.T @ A  # Shape: (n_features, n_features)

    # Step 2: Compute the top 'maxbits' eigenvectors and eigenvalues of ATA
    # Using scipy.linalg.eigh which returns them in ascending order, so we take the last 'maxbits'
    eigenvalues, pc = eigh(ATA)
    pc = pc[:, -maxbits:]  # Shape: (n_features, maxbits)
    pv = np.diag(eigenvalues[-maxbits:])  # Shape: (maxbits, maxbits)

    # Step 3: Compute the mean of the top 'maxbits' eigenvalues
    a = np.mean(np.diag(pv))

    # Step 5: Initialize R with a random orthogonal matrix
    R = np.random.randn(maxbits, maxbits)
    U11, _, _ = np.linalg.svd(R)
    R = U11[:, :maxbits]  # Shape: (maxbits, maxbits)

    # Step 6: Compute initial Z
    Z = R.T @ pv @ R  # Shape: (maxbits, maxbits)

    # Step 7: Iterative optimization
    for iter_num in range(n_iter):
        T = Z.copy()
        np.fill_diagonal(T, a)  # Set the diagonal elements to 'a'

        # Compute eigenvectors of T
        # Since T is symmetric, use eigh for efficiency
        eigenvalues_T, R = eigh(T)
        R = R[:, -maxbits:]  # Select the top 'maxbits' eigenvectors

        # Update Z
        Z = R.T @ pv @ R

    # Step 8: Compute Y and binary codes B
    Y = A @ pc @ R  # Shape: (n_samples, maxbits)
    B = (Y > 0).astype(int)  # Binary codes: 1 if Y > 0, else 0

    # Step 9: Prepare the model
    model = {
        'pc': pc,  # Principal components
        'R': R  # Rotation matrix
    }
    return model, B
